---
title: 《机器学习》笔记-线性模型（3）
date: 2018-01-25 08:52:10
tags: 机器学习笔记
---
由于图片外链被禁止了，图片不能显示，完整文章看这里吧：<https://zhuanlan.zhihu.com/p/33270877>

# 写在最前面
如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考[《机器学习/深度学习入门资料汇总》](https://zhuanlan.zhihu.com/p/30980999)），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。

# 章节目录
* 基本形式
* 线性回归
* 对数几率回归
* 线性判别分析
* 多分类学习
* 类别不平衡问

## （一）基本形式
给定d个属性描述示例x=(x1;x2;...;xd)，其中xi是x在第i个属性上的取值，线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数，即，
![3.1](http://upload-images.jianshu.io/upload_images/4905018-3b01efbc6f4f3398.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
一般用向量形式写成，
![3.2](http://upload-images.jianshu.io/upload_images/4905018-a72e2b67c4546560.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
其中，w=（w1;w2;...;wd）。w和b学得之后，模型就得以确定。

## （二）线性回归
给定数据集D={(x1,y1),(x2,y2),...,(xm,ym)}，其中，xi=(xi1;xi2;...;xid)，yi∈R。“线性回归”（linear regression）试图学得一个线性模型以尽可能准确的预测实际输出标记。
我们先考虑一种最简单的情况：输入属性的数目只有一个。线性回归试图学得，
![3.3](http://upload-images.jianshu.io/upload_images/4905018-bfff87632cc860c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如何确定w和b呢？显然，关键在于如何衡量f(x)与y之间的差别。第二章中介绍过，均方误差是回归任务中常用的性能度量，因此我们可以试图让均方误差最小化，即，
![3.4](http://upload-images.jianshu.io/upload_images/4905018-9f2d5064b445824a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称“欧式距离”（Euclidean distance）。基于均方误差最小化进行模型求解的方法称为“最小二乘法”（least square method）。在线性回归中，最小二乘法就是输入找到一条直线，使所有样本到直线上的欧式距离之和最小。
求解w和b使，
![期望](http://upload-images.jianshu.io/upload_images/4905018-86211addcc5e6aae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
最小化的过程，称为线性回归模型的最小二乘“参数估计”（parameter estimation）。我们可以将E(w,b)分别对w和b求导，得到，
![3.5，3.6](http://upload-images.jianshu.io/upload_images/4905018-a96176cb36d400ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
然后，领上面的式子为零，从而求得w和b的最优解，
![3.7](http://upload-images.jianshu.io/upload_images/4905018-3a3b8d5661f2f16f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![3.8](http://upload-images.jianshu.io/upload_images/4905018-758a199ddd864037.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
更一般的情况是数据集D，样本由d个属性描述。此时我们试图学得，
![样本多属性](http://upload-images.jianshu.io/upload_images/4905018-4dba383472b1915b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
这称为“多元线性回归”（multivariate linear regression）。
类似的，可利用最小二乘法来对w和b进行估计。为了便于讨论，我们把w和b吸入向量形式，
![w,b向量形式](http://upload-images.jianshu.io/upload_images/4905018-7aa8be8cca1e5cad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
相应的，把数据集D表示为一个mx(d+1)大小的矩阵X，其中，每行对应于一个示例，该行前d个元素对应于示例的d个属性值，最后一个元素恒置为1，即，
![矩阵X](http://upload-images.jianshu.io/upload_images/4905018-8548771c3151715b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
再把标记也写成向量形式y=(y1;y2;...;ym)，则有，
![3.9](http://upload-images.jianshu.io/upload_images/4905018-743937b3a5e9726c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
当[公式2-1]
![公式2-1](http://upload-images.jianshu.io/upload_images/4905018-98122131b0824eb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
为满秩矩阵（full-rank matrix）或正定矩阵（positive definite matrix）时，可求得，
![3.11](http://upload-images.jianshu.io/upload_images/4905018-56450bd27b55db05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
然而，显示任务中[公式2-1]往往不是满秩矩阵。例如许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致X的列数大于行数，[公式2-1]显然不满秩。此时可解出多个w，他们都能使均方误差最小化。选择哪一个最为输出，将由学习算法的归纳偏好决定，常见的做饭是引入正则化（regularization）项。
更一般地，考虑单调可微函数g(.)，令
![3.15](http://upload-images.jianshu.io/upload_images/4905018-0decc0db32a0f274.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
这样得到的模型称为“广义线性模型”（generalized linear model）。

## （三）对数几率回归
上一节讨论了如何使用线性模型进行回归学习，但若要做的是分类任务该怎么办？这里可以考虑广义线性模型：只要找到一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。
考虑二分任务，其输出标记y∈{0，1}，而线性回归模型产生的预测值，
![公式](http://upload-images.jianshu.io/upload_images/4905018-e60506522df5f8d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
是实值，于是，我们需将实值z转换为0/1值。最理想的是单位阶跃函数（unit-step function）。
但单位阶跃函数不连续，因此不能作为广义线性模型。于是我们希望找到能在一定程度上近似单位阶跃函数的“替代函数”（surrogate function），并希望它单调可微分。对数几率函数（logistic function）正是这样一个常用的替代函数（Sigmoid函数）：
![3.17](http://upload-images.jianshu.io/upload_images/4905018-7f7d5e98751e2451.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
即，
![3.18](http://upload-images.jianshu.io/upload_images/4905018-a55fcebcc3245fe7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

函数如下图所示，
![图3.2](http://upload-images.jianshu.io/upload_images/4905018-786444a5810b0d94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
下面我们来看如何确定w和b，
![3.23，3.24](http://upload-images.jianshu.io/upload_images/4905018-830a4b3fb420769b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
我们可以通过“极大似然法”（maximum likelihood method）来估计w和b，
![3.25](http://upload-images.jianshu.io/upload_images/4905018-07000a7ae1b8956c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
根据凸优化理论，经典的数值优化算法如梯度下降法（gradient descent method）、牛顿法（Newton method）都可以求得最优解。

## （四）线性判别分析
线性判别分析（Linear Discriminant Analysis，简称LDA）的思想非常朴素：给定训练样例集，设法将样例投影到一条直线上，使得同样样例的投影点尽可能接近，异类样例的投影点尽可能远离；对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置确定新样本的类别，如下图所示，
![图3.3](http://upload-images.jianshu.io/upload_images/4905018-4eec5600ee6fbfb0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
多分类LDA将样本投影到d'维空间，d'通常小于数据原有的属性数d,于是通过这个投影来减小样本点的维数，且投影过程中使用了类别信息，因此LDA也常被视为一种经典的监督降维技术。

## （五）多分类学习
现实中常遇到多分类学习任务。有些二分类学习方法可直接推广到多分类。
考虑N个类别C1,C2,...,CN，多分类学习的基本思路是“拆解法”，即将多分类任务拆为若干若干个二分类任务求解。具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器；在测试时，对这些分类器的预测结果进行集成以获得最终的分类结果。
最经典的分类拆分策略有三种：
* “一对一”（One vs One，简称OvO）
* “一对其余”（One vs Rest，简称OvR）
* “多对多”（Many vs Many）。

多分类过程如下图所示（OvO与OvR示意图），
![图3.4](http://upload-images.jianshu.io/upload_images/4905018-99f3730369210654.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## （六）类别不平衡问题
前面介绍的分类学习方法都有一个共同的基本假设，即不同类别的训练样例数目相当。如果不同类别的训练样例数目稍有差别，通常影响不大，但若差别很大，则会对学习过程造成困扰。
针对这种情况，现有技术上塔体有三类做法（假定正类样例较少，反例样例较多）：
* 第一类是直接对训练集里的反例样本进行“欠采样”（undersampling），即去除一些反例使得正、反例数目接近，然后在进行学习；
* 第二类是对训练集里的正类样例进行“过采样”（oversampling），即增加一些正例使得正、反例数目接近，然后再进行学习；
* 第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将“再缩放”（rescaling）嵌入到过程中，称为“阈值移动”（threshold-moving）；