---
title: 《机器学习》笔记-降维与度量学习（10）
date: 2018-02-10 10:47:15
tags: 机器学习笔记
---
由于图片外链被禁止了，图片不能显示，完整文章看这里吧：<https://zhuanlan.zhihu.com/p/33739227>

# 写在最前面
如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考[《机器学习/深度学习入门资料汇总》](https://zhuanlan.zhihu.com/p/30980999)），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。

# 章节目录

* k近邻学习
* 低纬嵌入
* 主成分分析
* 核化线性降维
* 流形学习
* 度量学习

## （一）k近邻学习
k近邻（k-Nearest，简称kNN）学习是一种常用的监督学习方法，其工作机制非常简单：给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本。然后基于这k个“邻居”的信息进行预测。通常，在分类任务中可使用“投票法”，即选择这k个样本中出现最多的类别标记作为预测结果；在回归任务中可使用“平均法”，即将这k个样本的实际值输出标记的平均值作为预测结果。还可基于距离远近的加权平均或加权投票，距离越近的样本权重越大。
与先前介绍的学习方法相比，k近邻学习有一个明显的不同之处，它似乎没有显示的训练过程。事实上，它是“懒惰学习”（lazy learning）的著名代表，此类学习技术在训练阶段仅仅是将样本存储起来，训练开销为零，待收到测试样本后再进行处理；相应的，那些在训练阶段就对样本进行学习处理的方法，称为“急切学习”（eager learning）。
下图给出了k近邻分类器的一个示意图，
![图10.1](http://upload-images.jianshu.io/upload_images/4905018-e32c732fa967b465.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
近邻分类器虽简单，但它的泛化错误率不超过贝叶斯最优分类器的错误率的两倍。

## （二）低维嵌入
上一节的讨论是基于一个重要假设：任意测试样本x附近任意小的δ距离范围内总能找到一个训练样本，即训练样本的采样密度足够大，或称为“密采样”（dense sample）。然而，这个假设在现实任务中通常很难满足。此外，许多学习方法都涉及距离计算，而高维空间会给距离计算带来很大麻烦，例如当维数很高时，甚至连计算内积都不再容易。
事实上，在高维情形下出现的数据样本稀疏，计算距离困难等问题，是所有机器学习方法共同面临的严重障碍，被称为“维数灾难”（curse of dimensionality）。
缓解维数灾难的一个重要途径是降维（dimension reduction），亦称“维数简约”，即通过某种数学变换将原始高纬度属性空间转变为一个低维“子空间”（subspace），在这个子空间中样本密度大幅提高，计算距离也变得更为容易。为什么能进行降维？这是因为在很多时候，人们人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间的一个底维“嵌入”（embedding）。下图给出了一个直观的例子，原始高维空间中的样本点，在这个低纬嵌入子空间中更容易进行学习。
![图10.2](http://upload-images.jianshu.io/upload_images/4905018-9301d899e9040095.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
若要求原始空间中的样本之间距离在低维空间中得以保持，如上图所示，即得到“多维缩放”（Multiple Dimensional Scaling，简称MDS）这样一种经典的降维方法。
对降维效果的评估，通常是比较降维前后学习器的性能，若性能有所提高则认为降维起到了作用。若维数降低到二维或三维，则可通过可视化技术来直观的判断降维效果。

## （三）主成分分析
主成分分析（Principal Component Analysis，简称PCA）是最常用的一种降维方法。
PCA仅需保留W与与样本的均值向量即可通过简单的向量减法和矩阵向量乘法将样本投影到低维空间中。显然，低维空间与原始高维空间必有不同，因为对应于最小的d-d'个特征值的特征向量被舍弃了，这是降维导致的结果。但舍弃这样的信息往往是必要的：
* 一方面，舍弃这部分信息之后能使样本的采样密度增大，这正是降维的重要动机；
* 另一方面，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到去噪的效果。

## （四）核化线性降维
线性降维方法假设从高维空间到低维空间的函数映射是线性的，然而，在不少实际任务中，可能需要非线性映射才能找到恰当的低维嵌入。
非线性降维的一种常用方法，是基于核技巧对线性降维方法进行”核化“（kernelized）。

## （五）流形学习
流形学习（manifold learning）是一类借鉴了拓扑流形概念的降维方法。”流形“是在局部与欧氏空间同胚的空间，换言之，它在局部具有欧氏空间的性质，能用欧氏距离来进行距离计算。这给降维方法带来了很大的启发。若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看上去非常复杂，但在局部上仍具有欧氏空间的性质，因此，可以容易的在局部建立降维映射关系，然后，再设法将局部映射关系推广到全局。当维数被降至二维或三维时，能对数据进行可视化展示，因此流形学习也可被用于可视化。
其中，等量度映射和局部线性嵌入式两种著名的流形学习方法。

## （六）度量学习
在机器学习中，对高维数据进行降维的主要目的是希望找到一个合适的低维空间，在此空间中进行学习能比原始空间性能更好。事实上，每个空间对应了在样本属性上定义的一个距离度量，而寻找合适的空间，实质上就是在寻找一个合适的距离度量。直接尝试”学习“出一个合适的距离度量，就是度量学习（metric learning）的基本动机。
